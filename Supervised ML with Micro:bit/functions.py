# I made this file to collect all the useful functions from different notebooks in one file.
#This file is going to be imported to Live_test_final.ipynb to carry out the live test of the ML model.

# import the necessary libraries
import os
import glob
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time as t
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_score
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D



# This function gets the files from the Data directory and returns a list of the absolute paths of the data files.
def get_files(filepath):
    all_files = []
    for root, dirs, files in os.walk(filepath):
        files = glob.glob(os.path.join(root, '*.txt'))
        for f in files:
            all_files.append(os.path.abspath(f))
            
        return all_files

# This function reads the file with given absolute path and turns the content to a Pandas data frame.
# Use this function after get_files. The function returns a Pandas data frame.
def process_file(datafile):
    myfile = open(datafile, 'r')
    datalist = myfile.readlines()
    data_to_frame = []
    for item in datalist:
        item = item.split(sep = ',')
        data_to_frame.append(item)
            
    df = pd.DataFrame(data = data_to_frame, columns = ['a_x', 'a_y', 'angle'])
    return df

# This function turns string data in the data frame to numeric format. Use this function after process_file.
# The function returns the same data frame with numeric data format.
def to_numeric(df):
    df['a_x'] = pd.to_numeric(df['a_x'])
    df['a_y'] = pd.to_numeric(df['a_y'])
    df['angle'] = pd.to_numeric(df['angle'])
    
    return df
    
# This function works out the range of the features in the data frame generated by to_numeric.
# Use this to build the final data set that will be used to train the model. r_x is generated differently, see the notebook.
# The function returns the tuple (r_x, r_y, r_ang).
def range_(df, id_):
    r_x = round(max(df['a_x'])-min(df['a_x']),1)
    r_y = round(max(df['a_y'])-min(df['a_y']),1)
    if id_ ==0:
        r_ang = np.abs(round(max(df['angle'])-min(df['angle']),1)-120)
    else:
        r_ang = round(max(df['angle'])-min(df['angle']),1)
        
    return r_x, r_y, r_ang


# This function is used to logaritmic transformation of the data found in the data frame created by to_numeric.
# It retruns the same data frame with the transformed data. 
def log_transform(df, features):
    temp_fr = pd.DataFrame(data = df[features])
    temp_fr_log = temp_fr.apply(lambda x: np.log(np.abs(x)))
    for feature in features:
        df[feature] = temp_fr_log[feature]
    return df

# This function is used to standardise the data before training the ML model.
# The function returns a numpy array with the standardised data.
def standardise(features):
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    return features_scaled

#This function is used to split the data into train and test data. X_train, X_test, y_train and y_test are numpy arrays.
# X is feaurte data, y is target data. Ratio is the proportion of the data which will be test data, ratio is a number between 0 and 1.
# The function returns the numpy arrays of the train and test feature and target data.
def split_data(data_scaled,target,ratio):
    X_train, X_test, y_train, y_test = train_test_split(data_scaled, target, test_size = ratio, random_state =42)
    
    print(f'The train set has {X_train.shape[0]} elements')
    print(f'The test set has {X_test.shape[0]} elements')
    
    return X_train, X_test, y_train, y_test


# This function is used to train different ML models and test its performance.
# Learner is the type of the model that will be trained, sample size the percentage of the size of the training dataset (e.g. 10% = 0.1).
# The function returns the list of the learner name, the number of used train samples, the maximum number of train samples and the different accuarcy scores.
def train_predict(learner,learner_name, sample_size, X_train, X_test, y_train, y_test):
    
    # train the model and make prediction
    start = t.time()
    sample_size = int(sample_size*X_train.shape[0])
    print(f'The training set has {sample_size} elements out of the {X_train.shape[0]} elements.')
    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])
    prediction_test = learner.predict(X_test)
    end = t.time()
    
    time_taken = end - start
    
    # add accuracy scores
    precision_score = precision(y_test, prediction_test)
    recall_score = recall(y_test, prediction_test)
    F1_score = f1(y_test, prediction_test)
    
    return [learner_name, sample_size, X_train.shape[0], precision_score, recall_score, F1_score, time_taken]

# This function is used to train the K-nearest model when live testing our project.
# data_path is the path to the file final_dataset. The function returns the trained ML model instance.
def train_K_model(dataset_path):
    # getting the dataset
    final_dataset = pd.read_csv(dataset_path)
    final_dataset.drop(columns = 'Unnamed: 0', inplace = True)
    print('A piece of the dataset we train the model on is: \n' ,final_dataset.head())
    
    # preprocessing the final dataset
    features = ['r_y', 'r_ang']
    final_dataset_log = log_transform(final_dataset, features)
    data_scaled = standardise(final_dataset_log[['r_x', 'r_y', 'r_ang']])
    print('The final dataset contains {} data instances'.format(data_scaled.shape[0]))
    
    # split the data into train and test dataset
    X_train, X_test, y_train, y_test = split_data(data_scaled, final_dataset['id'], 0.2)
    
    # train the model
    clas = KNeighborsClassifier(n_neighbors = 5, p = 2)
    clas = clas.fit(X_train, y_train)
    
    # test the model
    prediction_test = clas.predict(X_test)
    prediction_test
    prec = precision_score(y_true = y_test, y_pred= prediction_test, average = 'macro')
    print("The precision of the model we trained is : ", round(prec,3))
    return clas
    
# This function is used to find the mean and the standard deviation of the features in the final dataset.
# The function returns the tuple (mean, std deviation) that will be used when evaluating the test data instance (create_show_test_inst)
def mean_std(df,feature):
    mean = df[feature].mean()
    std = df[feature].std()
    
    return mean, std
    

# This function gets and processes the test data file, creates the test instance and shows it on the graph.
# This is the combination of many previous functions and it returns the test data instance.
def create_show_test_inst():
    #acces the data in the file
    file = get_files('Data_test')
    df = process_file(file[0])
    df = to_numeric(df)
    
    #create the test instance
    test_tuple = range_(df, df['a_x'][0])
    test_list = list(test_tuple)
    print(f"The test instance before transformation: {test_list}, and the data type is {type(test_list)}")
    test_list[1] = np.log(test_list[1])
    test_list[2] = np.log(test_list[2])
    print(f'The test instance after log transformation: {test_list} and the data type is {type(test_list)}')
   
    test_list[0] = (test_list[0]-18.4801)/7.295215291091856
    test_list[1] = (test_list[1]-2.51855636884485)/0.8019631364658939
    test_list[2] = (test_list[2]-4.119557817651554)/1.6615853623084749
    test_inst = np.full((1, 3), test_list)
    print(f'The test instance after the log transformation and standardisation is: {test_inst}, and the data type is {type(test_inst)}')
          

    
    # show the test isntance on the graph
    final_dataset = pd.read_csv('final_dataset.csv')
    final_dataset.drop(columns = 'Unnamed: 0', inplace = True) # load final dataset again because it has been log-transformed

    fig = plt.figure(figsize = (8,8))
    ax = fig.add_subplot(111,projection = '3d')

    ax.set_xlim(0,25)
    ax.set_ylim(25,0)
    ax.set_xlabel('r_x')
    ax.set_ylabel('r_y')
    ax.set_zlabel('r_ang')

    r_x = final_dataset['r_x']
    r_y = final_dataset['r_y']
    r_ang = final_dataset['r_ang']

    colours = ['r', 'g', 'b']

    for i in final_dataset.id.unique():
        ax.scatter(final_dataset.r_x[final_dataset.id == i], final_dataset.r_y[final_dataset.id == i], 
                   final_dataset.r_ang[final_dataset.id == i], c = colours[i])

    ax.scatter(test_tuple[0], test_tuple[1], test_tuple[2], c = 'k', s = 200)

    plt.show()
          
    # return the test_instance
    return test_inst

# This function is used to make a prediction with the trained K nearest model when live testing it.
# The function doesn't return an object but tells the prediction to the user. 
# dataset path is the path to the file final_dataset, test_inst is the test instance created by create_show_test_inst.
def predict(dataset_path, test_inst):
    
    # train the model
    clas = train_K_model(dataset_path)
    
    # predict the motion
    prediction = clas.predict(test_inst)
    print(f'prediction is {prediction}')
    if prediction == 0:
        motion = 'SHM'
    elif prediction == 1:
        motion = 'pendulum'
    elif prediction == 2:
        motion = 'circular'
    else:
        motion = 'unidentified'
    
    print(f'The motion I detected is a {motion} motion')


